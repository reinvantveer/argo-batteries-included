global:
  argoServerHost: &ref_argoServerHost
    argo.localhost

argo-cd:
  enabled: true
  dex:
    enabled: false

  server:
    extraArgs:
      - --insecure
    ingress:
      enabled: true
      hosts:
        - argocd.localhost

minio:
  enabled: true

  mode: standalone

  # Tweak the size of the disk usage for storage
  persistence:
    size: 5Gi

  replicas: 1

  resources:
    requests:
      memory: 128Mi

  consoleIngress:
    enabled: true
    path: /
    hosts:
      - minio.localhost

  users:
    ## Username, password and policy to be assigned to the user
    ## Default policies are [readonly|readwrite|writeonly|consoleAdmin|diagnostics]
    ## Add new policies as explained here https://docs.min.io/docs/minio-multi-user-quickstart-guide.html
    ## NOTE: this will fail if LDAP is enabled in your MinIO deployment
    ## make sure to disable this if you are using LDAP.
    - accessKey: console
      secretKey: console123
      policy: consoleAdmin

    # This user account is for reading from and writing to the Argo artifacts bucket from each namespace that Argo is
    # allowed to deploy workflows in.
    # NOTE that this user is expected to be the LAST one in the list of users!
    - accessKey: argo
      secretKey: argo-pass  # Should be at least 8 chars
      policy: readwrite

  # Bucket for Argo logging and data artifacts
  buckets:
   - name: argo
     policy: none
     purge: false

# Back-end for workflow archive
postgresql:
  enabled: true

  resources:
    requests:
      memory: 64Mi

  # On k3s, PostgreSQL somehow needs special permissions on the volume in order to set the user/pass config properly
  volumePermissions:
    enabled: true

  postgresqlUsername: postgres
  postgresqlPassword: postgres

# Argo Server and Argo Workflows configuration
argo-workflows:
  enabled: true

  workflow:
    rbac:
      create: true
    serviceAccount:
      create: true

  server:
    ingress:
      enabled: true
      hosts:
        - *ref_argoServerHost
      paths:
        - /

    extraArgs:
      # Disable dev-mode authentication
      - --auth-mode=server

    secure: false

  controller:
    # See also https://github.com/argoproj/argo-helm/tree/master/charts/argo-workflows#workflow-controller-1
    containerRuntimeExecutor: emissary

    workflowNamespaces:
      - operators
      - data

    workflowDefaults:
      archiveLogs: true
      serviceAccountName: argo-workflow

    # Set up database connection details for workflow archive
    persistence:
      connectionPool:
        maxIdleConns: 10
        maxOpenConns: 50
      # save the entire workflow into etcd and DB
      nodeStatusOffLoad: false
      # enable archiving of old workflows
      archive: true
      postgresql:
        host: argo-postgresql
        port: 5432
        database: postgres
        tableName: argo_workflows
        # the database secrets must be in the same namespace of the controller
        userNameSecret:
          name: argo-postgresql
          # NOTE that since we went with "postgres" as user, we only get a password secret, which is the same as the user
          key: postgresql-password
        passwordSecret:
          name: argo-postgresql
          key: postgresql-password

  # Set up logging artifact repo in minio s3 bucket
  # -- Influences the creation of the ConfigMap for the workflow-controller itself.
  useDefaultArtifactRepo: true
  # -- Use static credentials for S3 (eg. when not using AWS IRSA)
  useStaticCredentials: true
  artifactRepository:
    # -- Archive the main container logs as an artifact
    archiveLogs: true
    # -- Store artifact in a S3-compliant object store
    # @default -- See [values.yaml]
    s3:
      # Note the `key` attribute is not the actual secret, it's the PATH to
      # the contents in the associated secret, as defined by the `name` attribute.
      accessKeySecret:
        name: minio-bucket-access
        key: user
      secretKeySecret:
        name: minio-bucket-access
        key: pass
      insecure: true
      bucket: argo
      # See https://stackoverflow.com/questions/47027194/how-to-access-a-service-in-a-kubernetes-cluster-using-the-service-name#answer-47029021
      endpoint: argo-minio.operators.svc.cluster.local:9000
      # See also: https://github.com/argoproj/argo-workflows/blob/master/docs/workflow-controller-configmap.yaml#L101
      keyFormat: "logs\
              /{{workflow.creationTimestamp.Y}}\
              /{{workflow.creationTimestamp.m}}\
              /{{workflow.creationTimestamp.d}}\
              /{{workflow.name}}\
              /{{pod.name}}"